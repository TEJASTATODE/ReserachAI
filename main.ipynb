{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "712ffd29",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc626aec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_core.tools import tool\n",
    "from langchain.agents import create_agent\n",
    "from langgraph.checkpoint.memory import InMemorySaver\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "813ec752",
   "metadata": {},
   "outputs": [],
   "source": [
    "# llm=ChatGoogleGenerativeAI(\n",
    "#     model=\"gemini-2.5-flash\",\n",
    "#     temperature=0.2\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f5f53961",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = [\n",
    "    Document(\n",
    "        page_content=(\n",
    "            \"Retrieval Augmented Generation, or RAG, is a technique where a large \"\n",
    "            \"language model retrieves relevant information from an external \"\n",
    "            \"knowledge base and uses it as context to generate accurate answers.\"\n",
    "        )\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=(\n",
    "            \"LangChain is a framework that helps developers build applications \"\n",
    "            \"powered by large language models by chaining prompts, models, \"\n",
    "            \"retrievers, tools, and memory together.\"\n",
    "        )\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=(\n",
    "            \"Vector databases store numerical embeddings of text and allow \"\n",
    "            \"semantic similarity search instead of exact keyword matching.\"\n",
    "        )\n",
    "    )\n",
    "]\n",
    "\n",
    "\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=200,\n",
    "    chunk_overlap=20\n",
    ")\n",
    "chunks = splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4758c264",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=\"sentence-transformers/all-MiniLM-L6-v2\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9e3d713b",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore = FAISS.from_documents(chunks, embeddings)\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 3})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "0ed5639e",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "def knowledge_search(query: str) -> str:\n",
    "    \"\"\"\n",
    "    Search the knowledge base for factual or technical information.\n",
    "    \"\"\"\n",
    "    docs = retriever.invoke(query)\n",
    "\n",
    "    if not docs:\n",
    "        return \"I don't know\"\n",
    "\n",
    "    return \"\\n\".join(d.page_content for d in docs)\n",
    "\n",
    "@tool\n",
    "def summarize_text(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Summarize the given text in simple words.\n",
    "    \"\"\"\n",
    "    llm = ChatGoogleGenerativeAI(\n",
    "        model=\"gemini-1.5-flash\",\n",
    "        temperature=0.3\n",
    "    )\n",
    "    return llm.invoke(f\"Summarize this clearly:\\n{text}\").content\n",
    "\n",
    "@tool\n",
    "def calculator(expression: str) -> str:\n",
    "    \"\"\"\n",
    "    Evaluate a simple mathematical expression.\n",
    "    Example: '12 * 8'\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return str(eval(expression))\n",
    "    except Exception:\n",
    "        return \"Invalid mathematical expression\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "903f4423",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm=ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-2.5-flash\",\n",
    "    temperature=0.2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a95cd6ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt = ChatPromptTemplate.from_template(\n",
    "#     \"\"\"\n",
    "#     Use ONLY the context below to answer the question.\n",
    "#     If the answer is not in the context, say \"I don't know\".\n",
    "\n",
    "#     Context:\n",
    "#     {context}\n",
    "\n",
    "#     Question:\n",
    "#     {question}\n",
    "#     \"\"\"\n",
    "# )\n",
    "# output_parser = StrOutputParser()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "01fa7e88",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\n",
    "        \"system\",\n",
    "        \"\"\"\n",
    "        You are a Research AI assistant.\n",
    "\n",
    "        Rules:\n",
    "        - Use knowledge_search for factual or technical questions.\n",
    "        - Use summarize_text to simplify long answers.\n",
    "        - Use calculator for math calculations.\n",
    "        - If information is not found, say \"I don't know\".\n",
    "        - Do NOT hallucinate.\n",
    "        \"\"\"\n",
    "    ),\n",
    "    (\"human\", \"{input}\")\n",
    "])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "5d13c013",
   "metadata": {},
   "outputs": [],
   "source": [
    "# chain = prompt | llm | output_parser\n",
    "# def ask(question: str):\n",
    "#     docs = retriever.invoke(question)\n",
    "#     context = \"\\n\".join(d.page_content for d in docs)\n",
    "#     return chain.invoke({\"context\": context, \"question\": question})\n",
    "\n",
    "\n",
    "# print(ask(\"Explain retrieval augmented generation in simple terms\"))\n",
    "# print(ask(\"How does LangChain help in building LLM applications?\"))\n",
    "# print(ask(\"Tell me about vector databases\"))\n",
    "# print(ask(\"Who is Tejas?\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "7dbb2e71",
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = [knowledge_search, summarize_text, calculator]\n",
    "\n",
    "llm_with_tools = llm.bind_tools(tools)\n",
    "\n",
    "chain = agent_prompt | llm_with_tools\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "71313e4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def research_agent(question: str):\n",
    "    response = chain.invoke({\"input\": question})\n",
    "\n",
    "    if response.tool_calls:\n",
    "        for tool_call in response.tool_calls:\n",
    "            name = tool_call[\"name\"]\n",
    "            args = tool_call[\"args\"]\n",
    "\n",
    "            if name == \"knowledge_search\":\n",
    "                result = knowledge_search.invoke(args[\"query\"])\n",
    "                return result\n",
    "\n",
    "            if name == \"summarize_text\":\n",
    "                return summarize_text.invoke(args[\"text\"])\n",
    "\n",
    "            if name == \"calculator\":\n",
    "                return calculator.invoke(args[\"expression\"])\n",
    "\n",
    "    return response.content\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "e492e4f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieval Augmented Generation, or RAG, is a technique where a large language model retrieves relevant information from an external knowledge base and uses it as context to generate accurate answers.\n",
      "Vector databases store numerical embeddings of text and allow semantic similarity search instead of exact keyword matching.\n",
      "LangChain is a framework that helps developers build applications powered by large language models by chaining prompts, models, retrievers, tools, and memory together.\n",
      "LangChain is a framework that helps developers build applications powered by large language models by chaining prompts, models, retrievers, tools, and memory together.\n",
      "Retrieval Augmented Generation, or RAG, is a technique where a large language model retrieves relevant information from an external knowledge base and uses it as context to generate accurate answers.\n",
      "Vector databases store numerical embeddings of text and allow semantic similarity search instead of exact keyword matching.\n",
      "96\n",
      "LangChain is a framework that helps developers build applications powered by large language models by chaining prompts, models, retrievers, tools, and memory together.\n",
      "Retrieval Augmented Generation, or RAG, is a technique where a large language model retrieves relevant information from an external knowledge base and uses it as context to generate accurate answers.\n",
      "Vector databases store numerical embeddings of text and allow semantic similarity search instead of exact keyword matching.\n"
     ]
    }
   ],
   "source": [
    "print(research_agent(\"Explain retrieval augmented generation in simple terms\"))\n",
    "print(research_agent(\"Summarize what LangChain is\"))\n",
    "print(research_agent(\"What is 12 * 8?\"))\n",
    "print(research_agent(\"Who is Tejas?\"))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
